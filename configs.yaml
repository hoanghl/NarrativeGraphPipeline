# @package _global_

##################################
## GENERAL CONFIGURATIONS
##################################
work_dir: ${hydra:runtime.cwd}
data_dir: ${work_dir}/data/
print_config: True
ignore_warnings: True

defaults:
  # enable color logging
  - override hydra/hydra_logging: colorlog
  - override hydra/job_logging: colorlog
  - override /hydra/sweeper: optuna

optimized_metric: "valid/bleu_1"
hydra:
  run:
    dir: logs/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}

  sweeper:
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper
    storage: null
    study_name: initial_sweeper
    n_jobs: 1

    # 'minimize' or 'maximize' the objective
    direction: maximize

    # number of experiments that will be executed
    n_trials: 10

    # choose Optuna hyperparameter sampler
    # learn more here: https://optuna.readthedocs.io/en/stable/reference/samplers.html
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 48729
      consider_prior: true
      prior_weight: 1.0
      consider_magic_clip: true
      consider_endpoints: false
      n_startup_trials: 10
      n_ei_candidates: 24
      multivariate: false
      warn_independent_sampling: true

    # define range of hyperparameters
    search_space:
      batch_size:
        type: categorical
        choices: [4, 6, 8]
      model.w_decay:
        type: float
        low: 2e-2
        high: 0.95
      model.warmup_rate:
        type: float
        low: 0.1
        high: 0.25
      model.lr:
        type: float
        low: 1e-5
        high: 2e-2
      model.d_hid:
        type: categorical
        choices: [64, 128, 256, 384]
      model.num_layers_q:
        type: categorical
        choices: [2, 3, 4]
      model.num_layers_p:
        type: categorical
        choices: [2, 3, 4]
      model.num_layers_a:
        type: categorical
        choices: [2, 3, 4]
      model.dropout:
        type: float
        low: 0.1
        high: 0.25
      model.use_2_answers:
        type: categorical
        choices: [True, False]

##################################
## CONFIGURATIONS for components
##################################
PATH:
  utils: ""
  pretrained: ${PATH.utils}/bert-base-uncased
  data: ${PATH.utils}/processed/story/[SPLIT].parquet
  log: ${work_dir}/logs

mode: train
multigpu: False

batch_size: 5

##########################################
###### CONFIGURATIONS FOR EACH COMPONENT
##########################################
lq: 33
lc: 178
la: 10
nc: 10

sizes_dataset:
  train: 32747
  test: 10557
  valid: 3461

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 1
  precision: 32
  accelerator: null
  replace_sampler_ddp: False
  min_epochs: 1
  max_epochs: 50
  auto_lr_find: False
  terminate_on_nan: True
  log_every_n_steps: 40
  auto_scale_batch_size: False
  gradient_clip_val: 1.0
  check_val_every_n_epoch: 1
  reload_dataloaders_every_epoch: True
  weights_summary: "top"
  progress_bar_refresh_rate: 5
  resume_from_checkpoint: ${PATH.log}/last_best.ckpt

model:
  _target_: models.narrative_model.NarrativeModel
  batch_size: ${batch_size}
  lq: ${lq}
  lc: ${lc}
  la: ${la}
  num_heads: 4
  num_heads_persistent: 4
  num_layers_q: 2
  num_layers_p: 4
  num_layers_a: 4
  d_bert: 768
  d_hid: 256
  d_vocab: 30522
  lr: 1e-5
  w_decay: 0.9
  dropout: 0.2
  size_dataset_train: ${sizes_dataset.train}
  max_epochs: ${trainer.max_epochs}
  warmup_rate: 0.2
  path_pretrained: ${PATH.pretrained}
  path_test_pred: ${PATH.log}/pred.json
  path_train_pred: ${PATH.log}/train_pred.json
  path_valid_pred: ${PATH.log}/valid_pred.json
  use_2_answers: False

datamodule:
  _target_: datamodules.narrative_datamodule.NarrativeDataModule
  batch_size: ${batch_size}
  lc: ${lc}
  nc: ${nc}
  path_data: ${PATH.data}

callbacks:
  ModelCheckpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "valid/bleu_1"
    save_top_k: 2
    save_last: True
    mode: "max"
    dirpath: ckpts/
    filename: "narrative_weam-{epoch:02d}"
  LearningRateMonitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
  # EarlyStopping:
  #   _target_: pytorch_lightning.callbacks.EarlyStopping
  #   monitor: valid/bleu_1
  #   min_delta: 0.001
  #   patience: 4
  #   verbose: False
  #   mode: max

logger:
  tensorboard:
    _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    save_dir: tensorboard/
    name: ${now:%A %b-%d %H:%M}
    log_graph: False
    default_hp_metric: False
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    project: "narrative_weam"
    name: ${now:%A %b-%d %H:%M}_tune
    save_dir: "."
    offline: False # set True to store all logs only locally
    id: null # pass correct id to resume experiment!
    # entity: ""  # set to name of your wandb team or just remove it
    log_model: False
    prefix: ""
    job_type: "train"
    group: ""
    tags: []
